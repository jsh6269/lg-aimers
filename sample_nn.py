# -*- coding: utf-8 -*-
# Automatically generated by Colaboratory.

import random
from datetime import datetime

import numpy as np
import pandas as pd
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
from tqdm import tqdm_notebook
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split

random.seed(0)
np.random.seed(0)
torch.manual_seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'

from google.colab import drive
drive.mount('/gdrive')

train_data = pd.read_csv('/gdrive/My Drive/ML/mind/train.csv').drop([379, 24598], axis=0)
test_data = pd.read_csv('/gdrive/My Drive/ML/mind/test_x.csv')

from sklearn.preprocessing import MinMaxScaler

drop_list = [
             'QaE', 'QbE', 'QcE', 'QdE', 'QeE',
             'QfE', 'QgE', 'QhE', 'QiE', 'QjE',
             'QkE', 'QlE', 'QmE', 'QnE', 'QoE',
             'QpE', 'QqE', 'QrE', 'QsE', 'QtE',
             'index', 'hand']
replace_dict = {'education': str, 'engnat': str, 'married': str, 'urban': str}
train_y = train_data['voted']

A = [drop for drop in drop_list if 'E' in drop]
TEMP = train_data.loc[:, A]
TEMP_test = test_data.loc[:, A]
TEMP.describe()
scaler = MinMaxScaler()
scaler.fit(TEMP)
TEMP = pd.DataFrame(scaler.transform(TEMP))
TEMP_test = pd.DataFrame(scaler.transform(TEMP_test))
# TEMP.describe()

train_x = train_data.drop(drop_list + ['voted'], axis=1)
test_x = test_data.drop(drop_list, axis=1)
train_x = train_x.astype(replace_dict)
test_x = test_x.astype(replace_dict)
train_x = pd.get_dummies(train_x)
test_x = pd.get_dummies(test_x)
train_y = 2 - train_y.to_numpy()
train_x = train_x.to_numpy()
test_x = test_x.to_numpy()

print(train_data.shape)

# pd.DataFrame(train_x).describe()

train_x[:, :20] = (train_x[:, :20] - 3.) / 2.
test_x[:, :20] = (test_x[:, :20] - 3.) / 2
train_x[:, 20] = (train_x[:, 20] - 5.) / 5.
test_x[:, 20] = (test_x[:, 20] - 5.) / 5.
train_x[:, 21:31] = (train_x[:, 21:31] - 3.5) / 3.5
test_x[:, 21:31] = (test_x[:, 21:31] - 3.5) / 3.5

train_x2 = np.concatenate([train_x, TEMP], axis=1)
test_x2 = np.concatenate([test_x, TEMP_test], axis=1)

# E가 포함된 데이터셋을 학습에 사용하는 경우
train_y = torch.tensor(train_y, dtype=torch.float32)
train_x = torch.tensor(train_x2, dtype=torch.float32)
test_x = torch.tensor(test_x2, dtype=torch.float32)

N_MODEL = 100
N_EPOCH = 50
BATCH_SIZE = 128
LOADER_PARAM = {
    'batch_size': BATCH_SIZE,
    'num_workers': 4,
    'pin_memory': True
}

validate_bool = False
prediction = np.zeros((11383, 1), dtype=np.float32)

if validate_bool:
    train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.3, stratify=train_y)
    valid_len = len(valid_x)

train_len, test_len = len(train_x), len(test_x)

for N_EPOCH in [50]:
    for no in range(100):
        train_loader = DataLoader(TensorDataset(train_x, train_y),
                                  shuffle=True, drop_last=True, **LOADER_PARAM)
        if validate_bool:
            valid_loader = DataLoader(TensorDataset(valid_x, valid_y),
                                      shuffle=False, drop_last=False, **LOADER_PARAM)
        test_loader = DataLoader(TensorDataset(test_x, torch.zeros((test_len,), dtype=torch.float32)),
                                shuffle=False, drop_last=False, **LOADER_PARAM)
        print('train_shape :', train_x.shape)

        model = nn.Sequential(
            nn.Dropout(0.05),
            nn.Linear(111, 120, bias=False),
            nn.LeakyReLU(0.05, inplace=True),
            nn.Dropout(0.5),
            nn.Linear(120, 60, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(60, 1)
        ).to(DEVICE)
        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.20665], device=DEVICE))
        optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=4e-2)
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=N_EPOCH // 4, eta_min=1.2e-5)

        model.train()
        for epoch in tqdm_notebook(range(N_EPOCH), desc='{:02d}/{:02d}'.format(no + 1, N_MODEL)):
            for idx, (xx, yy) in enumerate(train_loader):
                optimizer.zero_grad()
                xx, yy = xx.to(DEVICE), yy.to(DEVICE)
                pred = model(xx).squeeze()
                loss = criterion(pred, yy)
                loss.backward()
                optimizer.step()
                scheduler.step(epoch + idx / len(train_loader))

        model.eval()
        prediction_valid = np.zeros((13659, 1), dtype=np.float32)

        if validate_bool:
            with torch.no_grad():
                for idx, (xx, yy) in enumerate(valid_loader):
                    xx, yy = xx.to(DEVICE), yy.to(DEVICE)
                    pred = model(xx)
                    prediction_valid[BATCH_SIZE * idx:min(BATCH_SIZE * (idx + 1), len(prediction_valid)), :] = pred[:, :]

                roc_auc = roc_auc_score(valid_y, prediction_valid)
                print('  roc_score : ', roc_auc)
                print()

        with torch.no_grad():
            for idx, (xx, _) in enumerate(test_loader):
                xx = xx.to(DEVICE)
                pred = (2. - torch.sigmoid(model(xx).detach().to('cpu'))).numpy()
                prediction[BATCH_SIZE * idx:min(BATCH_SIZE * (idx + 1), len(prediction)), :] \
                    += pred[:, :] / N_MODEL

df = pd.read_csv('/gdrive/My Drive/ML/mind/sample_submission.csv')

df.iloc[:, 1:] = prediction
df.to_csv('/gdrive/My Drive/Answer/NN/Boosting_real.csv', index=False)

print(df)
df.describe()
